---
title: "ADS-502-Group-6-Final_Project"
output: html_document
date: "`r Sys.Date()`"
 markdown: 
    wrap: sentence
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# ADS 502 Applied Data Mining Final Group Project

## Group 6

### Dip Raj Bista + Logan Van Dine + Ghassan Seba

*`This data mining project aims to assist a mobile phone company in estimating the price range of their products based on various features such as RAM, Internal Memory, etc. By analyzing historical sales data from different mobile phone companies, the project seeks to uncover relationships between multiple features and ‘selling price.’ The objective is to provide actionable insights enabling the company to price their mobile phones appropriately and become more competitive while utilizing a classification model.`*

### Import dataset and describe characteristics such as dimensions, data types, file types, missing data, and statistical description of data.

```{r}
# Load data files
mobileTrain <- read.csv("train.csv", header = TRUE)
mobileTest <- read.csv("test.csv", header = TRUE)
```

```{r}
# Find training data set dimensions
dimenTrain <- dim(mobileTrain)
print(dimenTrain)
```

```{r}
# Find testing data set dimensions
dimenTest <- dim(mobileTest)
print(dimenTest)
```

```{r}
# suppressMessages()
suppressMessages({
  
# Load conflicted package 
library(conflicted)
  
# Load the tidyverse package
library(tidyverse)

# Get the column names and data types
colnamesTrain <- colnames(mobileTrain)
dtypeTrain <- sapply(mobileTrain, class)

# Clean the output
colnamesTrain <- str_pad(colnamesTrain, 10, side = "left")
dtypeTrain <- str_pad(dtypeTrain, 10, side = "left")

# Combine the two into a single data frame
df <- tibble(
  colname = colnamesTrain,
  dtype = dtypeTrain
)

# Print the data frame
print(df)
})
```

```{r}
# Load the tidyverse package
library(tidyverse)

# Get the column names and data types
colnamesTest <- colnames(mobileTest)
dtypeTest <- sapply(mobileTest, class)

# Clean the output
colnamesTest <- str_pad(colnamesTest, 10, side = "left")
dtypeTest <- str_pad(dtypeTest, 10, side = "left")

# Combine the two into a single data frame
df <- tibble(
  colname = colnamesTest,
  dtype = dtypeTest
)

# Print the data frame
print(df)

```

```{r}
# Display summary statistics for the training data set
summaryTrain <- summary(mobileTrain)

print(summaryTrain)

```

```{r}
# Display summary statistics for the training data set
summaryTest<- summary(mobileTest)

print(summaryTest)
```

```{r}
# Get the sum of missing values for each column
missingTrain <- colSums(is.na(mobileTrain))

# Column names
trainColumnNames <- names(missingTrain)

# Create a new data frame with column names and missing value counts
missingTrainTable <- data.frame(Column = trainColumnNames, MissingTrainCount = missingTrain)

# Print the tidy table to the console
cat(sprintf("%-15s %-15s\n", "Column Name", "NA Count"))
cat("---------------------------\n")
for (i in seq_along(missingTrainTable$Column)) {
  cat(sprintf("%-15s %-15d\n", missingTrainTable$Column[i], missingTrainTable$MissingTrainCount[i]))
}

```

```{r}
# Get the sum of missing values for each column
missingTest <- colSums(is.na(mobileTest))

# Column names
testColumnNames <- names(missingTest)

# Create a new data frame with column names and missing value counts
missingTestTable <- data.frame(Column = testColumnNames, MissingTestCount = missingTest)

# Print the tidy table to the console
cat(sprintf("%-15s %-15s\n", "Column Name", "NA Count"))
cat("---------------------------\n")
for (i in seq_along(missingTestTable$Column)) {
  cat(sprintf("%-15s %-15d\n", missingTestTable$Column[i], missingTestTable$MissingTestCount[i]))
}

```

## Data Description:

### Categorical Variables:

-   *`blue`*: Binary categorical variable indicating whether the mobile has Bluetooth (0 or 1).
-   *`dual_sim`*: Binary categorical variable indicating whether the mobile has dual SIM capability (0 or 1).
-   *`four_g`*: Binary categorical variable indicating whether the mobile supports 4G (0 or 1).
-   *`three_g`*: Binary categorical variable indicating whether the mobile supports 3G (0 or 1).
-   *`touch_screen`*: Binary categorical variable indicating whether the mobile has a touch screen (0 or 1).
-   *`wifi`*: Binary categorical variable indicating whether the mobile has Wi-Fi capability (0 or 1).
-   *`price_range`*: TARGET VARIABLE; Categorical variable representing the price range of the mobile. It has discrete values (0, 1, 2, 3).

### Integer Variables:

-   *`battery_power`*: Discrete integer variable representing the battery power of the mobile.
-   *`fc`*: Discrete integer variable representing the front camera megapixels of the mobile.
-   *`int_memory`*: Discrete integer variable representing the internal memory in GB of the mobile.
-   *`mobile_wt`*: Discrete integer variable representing the weight of the mobile.
-   *`n_cores`*: Discrete integer variable representing the number of cores of the mobile's processor.
-   *`pc`*: Discrete integer variable representing the primary camera megapixels of the mobile.
-   *`px_height`*: Discrete integer variable representing the pixel height of the mobile.
-   *`px_width`*: Discrete integer variable representing the pixel width of the mobile.
-   *`ram`*: Discrete integer variable representing the RAM of the mobile in MB.
-   *`sc_h`*: Discrete integer variable representing the screen height of the mobile.
-   *`sc_w`*: Discrete integer variable representing the screen width of the mobile.
-   *`talk_time`*: Discrete integer variable representing the talk time of the mobile in hours.
-   *`id`*: Discrete integer variable representing the unique ID of the mobile (present in the test dataset).

### Numeric Variables:

-   *`clock_speed`*: Continuous numeric variable representing the clock speed of the mobile's processor.
-   *`m_dep`*: Continuous numeric variable representing the mobile depth.

### Data Distribution

```{r}
# Load necessary libraries
library(ggplot2)
library(tidyr)

# Define the number of rows and columns for the subplot matrix
num_rows <- 2
num_cols <- 7

# Select only the continuous and discrete integer columns from the mobileTrain dataset
selected_cols <- mobileTrain[, c("battery_power", "fc", "int_memory", "mobile_wt", 
                                 "n_cores", "pc", "px_height", "px_width", "ram", 
                                 "sc_h", "sc_w", "talk_time", "clock_speed", "m_dep")]

# Convert the selected data frame to long format
mobileTrain_long <- tidyr::pivot_longer(selected_cols, 
                                       cols = everything(),
                                       names_to = "name", values_to = "value")

# Convert the 'name' column to a factor or character variable
mobileTrain_long$name <- as.factor(mobileTrain_long$name)

# Create the boxplot using ggplot2 with mean markers
trainingPlots <- ggplot(mobileTrain_long, aes(x = name, y = value)) +
  geom_boxplot() +
  stat_boxplot(geom = "errorbar", width = 0.2, position = position_dodge(width = 0.75)) +
  stat_summary(fun = mean, geom = "point", shape = 5, size = 3, color = "red",
               position = position_dodge(width = 0.75)) +
  facet_wrap(~name, scales = "free_x", ncol = num_cols) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Print the plot
print(trainingPlots)
```
```{r}
# Comparison of Integer and Continuous Variables PRE Normalization

# Integer
hist(mobileTrain$int_memory, main="Internal Memory Megapixels Frequency of Mobile Phones", xlab="Internal Memory", col="lightblue")

hist(mobileTrain$ram, main="RAM Frequency of Mobile Phones", xlab="RAM", col="darkmagenta")

#Continuous
hist(mobileTrain$clock_speed, main="Clock Speed Frequency of Mobile Phones", xlab="Clock Speed", col="lightpink")
```
The box-and-whisker plots above prove that normalization is needed for the training data set. Normalization will allow for statistical analysis of all variables in relation to the price range variable on the same scale. Normalization will also allow a better visualization of outliers in the data set and the distribution of variables to be used.

The histograms also prove that normalization is necessary. For example, in a variable such as internal memory, the box-and-whisker plots gave no valuable information about the distribution.

### Normalize the training Data Set for proper analysis

# Perform Min-Max scaling

```{r}
# Load necessary libraries
library(dplyr)

# Min-Max scaling
min_max_norm <- function(x) {(x - min(x)) / (max(x) - min(x))}

#Apply normalization to necessacry columns (all columns - target)
normalized_mobileTrain <- as.data.frame(lapply(mobileTrain[1:20], min_max_norm))

#Add target variable (price_range) back
normalized_mobileTrain$price_range <- mobileTrain$price_range

#Validate normalization and target variable
head(normalized_mobileTrain)


```

### Plot normalized continuous variables in training data set

```{r}
# Load necessary libraries
library(ggplot2)
library(tidyr)

# Define the number of rows and columns for the subplot matrix
num_rows <- 2
num_cols <- 7

# Select only the continuous and discrete integer columns from the mobileTrain dataset
selected_cols <- normalized_mobileTrain[, c("battery_power", "fc", "int_memory", "mobile_wt", 
                                 "n_cores", "pc", "px_height", "px_width", "ram", 
                                 "sc_h", "sc_w", "talk_time", "clock_speed", "m_dep")]

# Convert the selected data frame to long format
mobileTrain_long <- tidyr::pivot_longer(selected_cols, 
                                       cols = everything(),
                                       names_to = "name", values_to = "value")

# Convert the 'name' column to a factor or character variable
mobileTrain_long$name <- as.factor(mobileTrain_long$name)

# Create the boxplot using ggplot2 with mean markers
trainingPlots <- ggplot(mobileTrain_long, aes(x = name, y = value)) +
  geom_boxplot() +
  stat_boxplot(geom = "errorbar", width = 0.2, position = position_dodge(width = 0.75)) +
  stat_summary(fun = mean, geom = "point", shape = 5, size = 3, color = "red",
               position = position_dodge(width = 0.75)) +
  facet_wrap(~name, scales = "free_x", ncol = num_cols) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Print the plot
print(trainingPlots)

```
After normalization, all numeric variables are on a scale 0 to 1. This action allows us to statistically analyze the distribution of variables in comparison to one another. For example, it is now known that battery_power, int_memory, mobile_wt, pc, px_width, ram, sc_h, and talk_time are normally distributed. Normalization also provides insight to which variables contain outliers in the training data set, fc and px_height.

Before correlations between variables can be determined, the outliers need to be identified and potentially removed.

### Identify outliers in the continuous variables via z-score

```{r}
# Function to calculate z-scores for a vector
calculate_z_scores <- function(x) {
  (x - mean(x)) / sd(x)
}

# Calculate z-scores for each column
z_scores <- mobileTrain_long %>%
  group_by(name) %>%
  mutate(z_score = calculate_z_scores(value))

# Set the z-score threshold for outlier detection
z_score_threshold <- 3

# Identify outliers based on z-scores
outliers <- z_scores %>%
  dplyr::filter(abs(z_score) > z_score_threshold)

# Print the outliers
print(outliers)
```

A common threshold for z-scores is 3 (positive or negative). With this knowledge, a threshold of 3 has been applied to the normalized numeric variable z-scores. After applying the threshold, we find that there are 12 records in the fc variable that are determined as outliers. As these records only account for 0.6% of the data, they will be removed in the next step.

### Remove Outliers from data set

```{r}
# Load necessary libraries
library(dplyr)

# Calculate z-scores for each column
z_scores <- apply(normalized_mobileTrain, 2, function(x) abs((x - mean(x)) / sd(x)))

# Find rows with z-scores greater than 3 in any column
outliers <- rowSums(z_scores > 3) > 0

# Filter out the outliers from the dataset
filtered_mobileTrain <- normalized_mobileTrain[!outliers, ]

# get dimensions of the filtered dataset
dim(filtered_mobileTrain)

```

### Verify Impact of Outliers Removal

```{r}
# Load necessary libraries
library(dplyr)

# Calculate z-scores for each column
z_scores <- apply(normalized_mobileTrain, 2, function(x) abs((x - mean(x)) / sd(x)))

# Find rows with z-scores greater than 3 in any column
outliers <- rowSums(z_scores > 3) > 0

# Filter out the outliers from the dataset
filtered_mobileTrain <- normalized_mobileTrain[!outliers, ]

# Print summary statistics of the original dataset
print(summary(normalized_mobileTrain))

# Print summary statistics of the filtered dataset
print(summary(filtered_mobileTrain))
```
With only 0.6% of the data being removed, there is nota large difference in distribution and summary statistics of variables after removal of the fc outliers.

### Plot filtered data set to check for outliers

```{r}
library(ggplot2)

# Define the number of rows and columns for the subplot matrix
num_rows <- 2
num_cols <- 7

numeric_vars <- c("battery_power","clock_speed", "fc", "int_memory", "m_dep",	"mobile_wt", "n_cores","pc", "px_height",	"px_width", "ram", "sc_h", "sc_w", "talk_time")

# Select only the numeric variables from the filtered_mobileTrain dataset
selected_cols <- filtered_mobileTrain %>%
  select(all_of(numeric_vars))

# Convert the column names to a factor or character variable
selected_cols <- selected_cols %>%
  gather(name, value)

# Create the boxplot using ggplot2 with mean markers
trainingPlotsFiltered <- ggplot(selected_cols, aes(x = name, y = value)) +
  geom_boxplot() +
  stat_boxplot(geom = "errorbar", width = 0.2, position = position_dodge(width = 0.75)) +
  stat_summary(fun = mean, geom = "point", shape = 5, size = 3, color = "red",
               position = position_dodge(width = 0.75)) +
  facet_wrap(~name, scales = "free_x", ncol = num_cols) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Print the plot
print(trainingPlotsFiltered)

```

### Distribution of Binary Variables

```{r}
# Load necessary libraries
library(ggplot2)

# Select only the binary columns from the normalized_mobileTrain dataset
binary_cols <- filtered_mobileTrain[, c("blue", "dual_sim", "four_g", "three_g", "touch_screen", "wifi")]

# Get the table of counts for each binary variable
binary_counts <- lapply(binary_cols, table)

# Combine the counts into a single data frame
binary_df <- do.call(rbind, lapply(names(binary_counts), function(var_name) {
  data.frame(Variable = rep(var_name, 2),
             Value = as.factor(c("0", "1")),
             Count = as.numeric(binary_counts[[var_name]]))
}))

# Create the heatmap using ggplot2
heatmap_plot <- ggplot(binary_df, aes(x = Variable, y = Value, fill = Count)) +
  geom_tile(color = "white") +
  scale_fill_gradient(low = "lightblue", high = "darkblue") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Print the plot
print(heatmap_plot)
```
### Correlation Matrix of Numeric Variables 

```{r}
# Library

library(Hmisc)

# Correlation matrix of all Numeric Variables

#Data Frame of Numeric Variables

corr_train_data <- filtered_mobileTrain %>%
  select(battery_power, fc, int_memory, mobile_wt, n_cores, pc, px_height, px_width, ram, sc_h, sc_w, talk_time, clock_speed, m_dep)

# Correlation Matrix followed by p-value matrix

#corr_train_data.cor = cor(corr_train_data)
#corr_train_data.cor #commented out to avoid redundancy

p_correlation = rcorr(as.matrix(corr_train_data))
p_correlation
```
```{r}

# Heatmap of Integer Correlation

palette = colorRampPalette(c("lightgreen", "white", "red")) (30)
heatmap(x = corr_train_data.cor, col = palette, sym = TRUE)
```
The correlation matrix and heatmap of coefficients do not prove any drastically strong relationships between variables. In terms of multicollinearity, this is a positive for model creation as variables are not too strongly related to predict the target variable. However, in terms of feature selection, there are no obvious selections to be made. A p-value matrix was created to indulge in further statistical analysis. Features will partially be selected based on the statistical significance of the relationship to other variables; not overbearingly strong but not non-existent.

### Numeric Variable Covariance Analysis to identify strongly correlated features

```{r}
# Load necessary libraries
library(dplyr)

# Define the numeric and categorical variables
numeric_vars <- c("battery_power", "fc", "int_memory", "mobile_wt", "n_cores", "pc", "px_height", "px_width", "ram", "sc_h", "sc_w", "talk_time", "clock_speed", "m_dep")
categorical_vars <- c("blue", "dual_sim", "four_g", "three_g", "touch_screen", "wifi")

# Perform Min-Max scaling and convert categorical variables to factors
filtered_factored_mobileTrain <- filtered_mobileTrain %>%
  mutate_at(vars(all_of(categorical_vars)), as.factor) %>%
  mutate(across(all_of(numeric_vars), ~ (.-min(.)) / (max(.) - min(.))))

# Calculate the covariance matrix for numeric variables
cov_matrix <- cov(filtered_factored_mobileTrain[, numeric_vars])

# Print covariance matrix with variable names
cat("Covariance Matrix for Numeric Variables:\n")
print(cov_matrix)


```

### Covariance Matrix Heatmap

```{r}
# Load necessary libraries
library(dplyr)
library(ggplot2)
library(reshape2)  # for melt function

# Define the numeric and categorical variables
numeric_vars <- c("battery_power", "fc", "int_memory", "mobile_wt", "n_cores", "pc", "px_height", "px_width", "ram", "sc_h", "sc_w", "talk_time", "clock_speed", "m_dep")
categorical_vars <- c("blue", "dual_sim", "four_g", "three_g", "touch_screen", "wifi")

# Perform Min-Max scaling and convert categorical variables to factors
filtered_factored_mobileTrain <- filtered_mobileTrain %>%
  mutate_at(vars(all_of(categorical_vars)), as.factor) %>%
  mutate(across(all_of(numeric_vars), ~ (.-min(.)) / (max(.) - min(.))))

# Calculate the correlation matrix for numeric variables
cov_matrix <- cov(filtered_factored_mobileTrain[, numeric_vars])

# Create a heatmap of the correlation matrix
heatmap_data <- as.data.frame(as.table(cov_matrix))
colnames(heatmap_data) <- c("Variable_1", "Variable_2", "Covariance")  # Replace spaces with underscores

# Create the heatmap
ggplot(heatmap_data, aes(x = Variable_1, y = Variable_2, fill = Covariance)) +
  geom_tile() +
  scale_fill_gradient2(low = "white", mid = "blue", high = "yellow", midpoint = 0) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))+
  labs(title = "Covariance Heatmap for Numeric Variables")

```

### Correlation with Target Variable

*`Higher absolute correlation values indicate potentially important features.`*

```{r}
# Calculate the correlation with the target variable
cor_with_target <- sapply(numeric_vars, function(var) cor(filtered_mobileTrain[[var]], filtered_mobileTrain$price_range))

# Print correlation values with the target variable neatly
cat("Correlation with the target variable (price_range):\n")
for (i in 1:length(numeric_vars)) {
  cat(numeric_vars[i], ": ", cor_with_target[i], "\n")
}
```

### Visualize the correlations between the numeric variables and the target variable "price_range" using a bar plot

```{r}
# Calculate the correlation with the target variable
cor_with_target <- sapply(numeric_vars, function(var) cor(filtered_mobileTrain[[var]], filtered_mobileTrain$price_range))

# Create a data frame for plotting
cor_df <- data.frame(Variable = numeric_vars, Correlation = cor_with_target)

# Plot correlation values with the target variable using a bar plot
library(ggplot2)

ggplot(cor_df, aes(x = reorder(Variable, -Correlation), y = Correlation)) +
  geom_bar(stat = "identity", fill = "dodgerblue") +
  coord_flip() +
  labs(title = "Correlation with the target variable (price_range)",
       x = "Numeric Variable",
       y = "Correlation") +
  theme_minimal()

```
### Binary Variables in relation to Price Range (Target Variable)

```{r}
# Counts and Contingency Tables for Binary Variables

# Bluetooth variable
blue_table <- table(filtered_mobileTrain$blue, filtered_mobileTrain$price_range)
row.names(blue_table) <- c("Has Bluetooth", "No Bluetooth")
blue_table

# Dual Sim
dual_sim_table <- table(filtered_mobileTrain$dual_sim, filtered_mobileTrain$price_range)
row.names(dual_sim_table) <- c("Has Dual Sim", "No Dual Sim")
dual_sim_table

# 4G 
fourG_table <- table(filtered_mobileTrain$four_g, filtered_mobileTrain$price_range)
row.names(fourG_table) <- c("Has 4G", "No 4G")
fourG_table

#3G
threeG_table <- table(filtered_mobileTrain$three_g, filtered_mobileTrain$price_range)
row.names(threeG_table) <- c("Has 3G", "No 3G")
threeG_table

# Touch Screen
touchscreen_table <- table(filtered_mobileTrain$touch_screen, filtered_mobileTrain$price_range)
row.names(touchscreen_table) <- c("Has Touch Screen", "No Touch Screen")
touchscreen_table

# Wifi
wifi_table <- table(filtered_mobileTrain$wifi, filtered_mobileTrain$price_range)
row.names(wifi_table) <- c("Has Wifi Capability", "No Wifi Capability")
wifi_table
```
```{r}
# Bar graphs with price range overlay for each binary variable

# Overlay Bargraphs

ggplot(filtered_factored_mobileTrain, aes(price_range)) + geom_bar(aes(fill = blue)) + ggtitle("Price Range with Bluetooth Overlay")

ggplot(filtered_factored_mobileTrain, aes(price_range)) + geom_bar(aes(fill = dual_sim)) + ggtitle("Price Range with Dual Sim Overlay")

ggplot(filtered_factored_mobileTrain, aes(price_range)) + geom_bar(aes(fill = four_g)) + ggtitle("Price Range with 4G Overlay")

ggplot(filtered_factored_mobileTrain, aes(price_range)) + geom_bar(aes(fill = three_g)) + ggtitle("Price Range with 3G Overlay")

ggplot(filtered_factored_mobileTrain, aes(price_range)) + geom_bar(aes(fill = touch_screen)) + ggtitle("Price Range with Touch Screen Overlay")

ggplot(filtered_factored_mobileTrain, aes(price_range)) + geom_bar(aes(fill = wifi)) + ggtitle("Price Range with Wifi Overlay")
```
### Chi-Squared Test on Binary Variables against Target variable

```{r}

```

### Feature Selection

Explanation (p-value, covariance, similar variables, business rules, etc.)

4G, touch screen, wifi = domain knowledge/business rules

RAM, battery_power = correlation w/ target variable

px_width, sc_w = covariance is very similar to heights, no loss with those variables, simultaneously more correlated to price (px_width is also normally distributed)

pc = highest correlation with fc, normally distributed, simultaneously higher correlation to price, no loss in fc (same function in mobile phone)

talk_time, mobile_wt, internal_memory = normally distributed variables, low correlations across variables (as phone gets                                            lighter, phone gets more expensive)

REMOVE blue, dual_sim, three_g, fc, n_cores, px_height, sc_h, clock_speed, m_dep

```{r}
# Remove variables that will not be used in model

final_mobileTrain = subset(filtered_mobileTrain, select = -c(blue, dual_sim, three_g, fc, n_cores, px_height, sc_h, clock_speed, m_dep) )

```

### Hypothesis

Ho - Null hypothesis, there is no relationship between predictor variables in final_mobileTrain and target variable, price_range

Ha - Alternative hypothesis, contradicts null, there is a statistically significant relationship between the target variables

"model results reject ___ hypothesis in favor of ___ hypothesis"

### Baseline Model (Biggest Category Model "Data Science Using Python and R")

```{r}
#Find totals of each possible price range outcome in final training set
sum(final_mobileTrain$price_range == "0")

sum(final_mobileTrain$price_range == "1")

sum(final_mobileTrain$price_range == "2")

sum(final_mobileTrain$price_range == "3")
```

```{r}
#Find total records in final training set
dim(final_mobileTrain)
```

```{r}
#Divide counts by total records for percentages
(496/1988)*100

(497/1988)*100

(497/1988)*100

(498/1988)*100

#Baseline model assigns all records to biggest category percentage
```
Baseline model has an accuracy of 25.05%.

### Classification Model with Training Set

### Validate Model on Test Set

```{r}
# Normalize Test Data

# Load necessary libraries
library(dplyr)

# Min-Max scaling
min_max_norm <- function(x) {(x - min(x)) / (max(x) - min(x))}

#Apply normalization to necessacry columns (all columns - target)
normalized_mobileTest <- as.data.frame(lapply(mobileTest[1:20], min_max_norm))

#Add target variable (price_range) back
normalized_mobileTest$price_range <- mobileTest$price_range

#Validate normalization and target variable
head(normalized_mobileTest)

#Remove Variables not being modeled + id
final_mobileTest = subset(filtered_mobileTrain, select = -c(xxxxx) )
```

### Evaluation Metrics on Model


